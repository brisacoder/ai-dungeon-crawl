{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a48a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import math\n",
    "import random\n",
    "from itertools import count\n",
    "from enum import Enum, auto\n",
    "from dataclasses import dataclass, field\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1aba9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "@dataclass\n",
    "class HyperParams:\n",
    "    batch_size: int = 64  # Larger batch for more stable learning\n",
    "    gamma: float = 0.95   # Slightly lower discount for faster learning\n",
    "    eps_start: float = 1.0\n",
    "    eps_end: float = 0.02   # Keep more exploration\n",
    "    eps_decay: int = 15000  # Much slower epsilon decay\n",
    "    tau: float = 0.001      # Slower target network updates\n",
    "    lr: float = 5e-4        # Higher learning rate\n",
    "    device: torch.device = field(default_factory=lambda: device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d868bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # Validate and parse the observation space\n",
    "        # Expected: 25 (5x5 grid) + 2 (position) = 27 total observations\n",
    "        self.vision_size = 25  # 5x5 local view\n",
    "        self.pos_size = 2      # normalized row, col position\n",
    "        \n",
    "        expected_obs = self.vision_size + self.pos_size\n",
    "        if n_observations != expected_obs:\n",
    "            raise ValueError(f\"Expected {expected_obs} observations (25 vision + 2 position), got {n_observations}\")\n",
    "        \n",
    "        # Convolutional layers for spatial understanding of 5x5 local view\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # 5x5 -> 5x5\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # 5x5 -> 5x5\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Calculate conv output size: 64 channels * 5 * 5 = 1600\n",
    "        conv_output_size = 64 * 5 * 5\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(conv_output_size + self.pos_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, n_actions)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Split input: first 25 elements are 5x5 grid, last 2 are position\n",
    "        grid_input = x[:, :self.vision_size].view(batch_size, 1, 5, 5)\n",
    "        pos_input = x[:, self.vision_size:]\n",
    "        \n",
    "        # Process spatial information with convolutions\n",
    "        grid_features = F.relu(self.bn1(self.conv1(grid_input)))\n",
    "        grid_features = F.relu(self.bn2(self.conv2(grid_features)))\n",
    "        \n",
    "        # Flatten conv output\n",
    "        grid_features = grid_features.view(batch_size, -1)\n",
    "        \n",
    "        # Combine spatial and positional features\n",
    "        combined = torch.cat([grid_features, pos_input], dim=1)\n",
    "        \n",
    "        # Process through fully connected layers\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0ac2a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepResult(Enum):\n",
    "    MOVED = 0.1           # Small positive reward for any movement\n",
    "    HIT_WALL = -0.2       # Wall collision penalty  \n",
    "    GOAL_REACHED = 50.0   # Higher goal reward\n",
    "    INVALID = -0.1        # Invalid action penalty\n",
    "\n",
    "class ExplorationRewards:\n",
    "    NEW_AREA_BONUS = 1.0    # Larger exploration bonus\n",
    "    REVISIT_PENALTY = -0.05  # Smaller backtracking penalty\n",
    "    DISTANCE_BONUS = 0.1     # Bonus for getting closer to goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7ab166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_movement_reward(is_new_area: bool, old_distance: int = None, new_distance: int = None) -> float:\n",
    "    \"\"\"Compute reward for valid movement based on exploration status and goal distance.\"\"\"\n",
    "    base_reward = StepResult.MOVED.value\n",
    "    \n",
    "    # Exploration bonus/penalty\n",
    "    if is_new_area:\n",
    "        exploration_reward = ExplorationRewards.NEW_AREA_BONUS\n",
    "    else:\n",
    "        exploration_reward = ExplorationRewards.REVISIT_PENALTY\n",
    "    \n",
    "    # Distance-based reward shaping (optional)\n",
    "    distance_reward = 0.0\n",
    "    if old_distance is not None and new_distance is not None:\n",
    "        if new_distance < old_distance:\n",
    "            distance_reward = ExplorationRewards.DISTANCE_BONUS  # Getting closer\n",
    "        elif new_distance > old_distance:\n",
    "            distance_reward = -ExplorationRewards.DISTANCE_BONUS  # Getting farther\n",
    "        # If distance unchanged, distance_reward remains 0\n",
    "    \n",
    "    return base_reward + exploration_reward + distance_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de5c1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actions(Enum):\n",
    "    \"\"\"Enum for actions in the dungeon environment.\"\"\"\n",
    "    MOVE_UP = 0\n",
    "    MOVE_DOWN = 1\n",
    "    MOVE_LEFT = 2\n",
    "    MOVE_RIGHT = 3\n",
    "    ATTACK = 4\n",
    "    PICKUP_ITEM = 5\n",
    "    USE_ITEM = 6\n",
    "    OPEN_INVENTORY = 7\n",
    "    QUIT = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0800430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, policy_net, hp):\n",
    "        self.policy_net = policy_net\n",
    "        self.hp = hp\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def select_action(self, state, action_space):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.hp.eps_end + (self.hp.eps_start - self.hp.eps_end) * \\\n",
    "            math.exp(-1. * self.steps_done / self.hp.eps_decay)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            self.policy_net.eval()  # Set to eval mode for inference\n",
    "            with torch.no_grad():\n",
    "                action = self.policy_net(state).max(1).indices.view(1, 1)\n",
    "            self.policy_net.train()  # Set back to training mode\n",
    "            return action\n",
    "        else:\n",
    "            return torch.tensor([[random.choice(action_space)]], device=self.hp.device, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1d8aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridCellValues(Enum):\n",
    "    EMPTY = 0\n",
    "    WALL = 1\n",
    "    VISITED = 2  # Breadcrumb system\n",
    "    GOAL = 3\n",
    "    PATH = 4   \n",
    "    START = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b10552dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DungeonEnv:\n",
    "    def __init__(self, rows=8, cols=8, wall_prob=0.25, seed=None, device=torch.device(\"cpu\")):\n",
    "        self.action_space = [a.value for a in Actions if a.value < Actions.ATTACK.value]\n",
    "        \n",
    "        # Local observation: 5x5 window around agent + agent position\n",
    "        self.vision_range = 2  # Agent can see 2 cells in each direction\n",
    "        self.obs_size = (2 * self.vision_range + 1) ** 2  # 5x5 = 25 cells\n",
    "        self.observation_space = self.obs_size + 2  # +2 for agent's row,col position\n",
    "        \n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.wall_prob = wall_prob\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.grid = None\n",
    "        self.visited_grid = None  # Track where agent has been\n",
    "        self.start_pos = None\n",
    "        self.goal_pos = None\n",
    "        self.agent_pos = None\n",
    "        self.device = device\n",
    "        self.max_steps = rows * cols * 3  # More steps for larger exploration\n",
    "        self.steps_taken = 0\n",
    "        self.reset()\n",
    "     \n",
    "    def get_local_observation(self):\n",
    "        \"\"\"Get local 5x5 observation around agent plus position info.\"\"\"\n",
    "        agent_row, agent_col = self.agent_pos\n",
    "        \n",
    "        # Create local observation window\n",
    "        local_obs = []\n",
    "        \n",
    "        for dr in range(-self.vision_range, self.vision_range + 1):\n",
    "            for dc in range(-self.vision_range, self.vision_range + 1):\n",
    "                obs_row = agent_row + dr\n",
    "                obs_col = agent_col + dc\n",
    "                \n",
    "                # Check bounds\n",
    "                if (0 <= obs_row < self.rows and 0 <= obs_col < self.cols):\n",
    "                    cell_value = self.grid[obs_row, obs_col]\n",
    "                    # Convert to float and normalize\n",
    "                    if cell_value == GridCellValues.EMPTY:\n",
    "                        if self.visited_grid[obs_row, obs_col]:\n",
    "                            local_obs.append(0.3)  # Visited empty space\n",
    "                        else:\n",
    "                            local_obs.append(0.0)  # Unvisited empty space\n",
    "                    elif cell_value == GridCellValues.WALL:\n",
    "                        local_obs.append(1.0)  # Wall\n",
    "                    elif cell_value == GridCellValues.GOAL:\n",
    "                        local_obs.append(0.9)  # Goal (if visible)\n",
    "                    elif cell_value == GridCellValues.START:\n",
    "                        local_obs.append(0.1)  # Start position\n",
    "                    else:\n",
    "                        local_obs.append(0.0)  # Default\n",
    "                else:\n",
    "                    # Out of bounds = wall\n",
    "                    local_obs.append(1.0)\n",
    "        \n",
    "        # Add agent's normalized position (helps with spatial awareness)\n",
    "        norm_row = agent_row / (self.rows - 1)\n",
    "        norm_col = agent_col / (self.cols - 1)\n",
    "        \n",
    "        # Combine local view + position\n",
    "        full_obs = local_obs + [norm_row, norm_col]\n",
    "        \n",
    "        return torch.tensor(full_obs, device=self.device, dtype=torch.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment and return initial state.\"\"\"\n",
    "        self.steps_taken = 0\n",
    "        \n",
    "        # Initialize visited grid\n",
    "        self.visited_grid = np.zeros((self.rows, self.cols), dtype=bool)\n",
    "        \n",
    "        # Generate maze-like grid\n",
    "        max_retries = 10\n",
    "        for attempt in range(max_retries):\n",
    "            self.grid = self.rng.choice(\n",
    "                [GridCellValues.EMPTY, GridCellValues.WALL],\n",
    "                (self.rows, self.cols), \n",
    "                p=[1 - self.wall_prob, self.wall_prob]\n",
    "            ) \n",
    "            \n",
    "            # Ensure borders are mostly walls for maze feel (except for some openings)\n",
    "            for i in range(self.rows):\n",
    "                for j in range(self.cols):\n",
    "                    if (i == 0 or i == self.rows-1 or j == 0 or j == self.cols-1):\n",
    "                        if self.rng.random() < 0.8:  # 80% chance of border walls\n",
    "                            self.grid[i, j] = GridCellValues.WALL\n",
    "            \n",
    "            # Find all empty cells\n",
    "            empty_cells = list(zip(*np.where(self.grid == GridCellValues.EMPTY)))\n",
    "            \n",
    "            if len(empty_cells) >= 2:\n",
    "                break\n",
    "            elif attempt == max_retries - 1:\n",
    "                # Last resort: create simple maze\n",
    "                self.grid.fill(GridCellValues.EMPTY)\n",
    "                # Add some walls randomly but ensure connectivity\n",
    "                for i in range(1, self.rows-1):\n",
    "                    for j in range(1, self.cols-1):\n",
    "                        if self.rng.random() < 0.3:\n",
    "                            self.grid[i, j] = GridCellValues.WALL\n",
    "                empty_cells = list(zip(*np.where(self.grid == GridCellValues.EMPTY)))\n",
    "                break\n",
    "        \n",
    "        # Place start and goal far apart\n",
    "        if len(empty_cells) >= 2:\n",
    "            # Try to maximize distance between start and goal\n",
    "            max_dist = 0\n",
    "            best_start = None\n",
    "            best_goal = None\n",
    "            \n",
    "            for i, start_cell in enumerate(empty_cells):\n",
    "                for j, goal_cell in enumerate(empty_cells):\n",
    "                    if i != j:\n",
    "                        dist = abs(start_cell[0] - goal_cell[0]) + abs(start_cell[1] - goal_cell[1])\n",
    "                        if dist > max_dist:\n",
    "                            max_dist = dist\n",
    "                            best_start = start_cell\n",
    "                            best_goal = goal_cell\n",
    "            \n",
    "            if best_start and best_goal:\n",
    "                self.start_pos = best_start\n",
    "                self.goal_pos = best_goal\n",
    "            else:\n",
    "                # Fallback\n",
    "                self.start_pos = empty_cells[0]\n",
    "                self.goal_pos = empty_cells[-1]\n",
    "        else:\n",
    "            # Should not happen with our fallback, but just in case\n",
    "            self.start_pos = (1, 1)\n",
    "            self.goal_pos = (self.rows-2, self.cols-2)\n",
    "\n",
    "        # Set positions in grid\n",
    "        self.grid[self.goal_pos] = GridCellValues.GOAL\n",
    "        self.grid[self.start_pos] = GridCellValues.START\n",
    "        self.agent_pos = self.start_pos \n",
    "        \n",
    "        # Mark starting position as visited\n",
    "        self.visited_grid[self.start_pos] = True\n",
    "        \n",
    "        return self.get_local_observation()\n",
    "\n",
    "    def step(self, action: int):\n",
    "        \"\"\"Apply action and return (next_state, reward, done, info).\"\"\"\n",
    "        try:\n",
    "            action_enum = Actions(action)\n",
    "        except ValueError:\n",
    "            reward = StepResult.INVALID.value\n",
    "            done = False\n",
    "            new_state = self.get_local_observation()\n",
    "            return new_state, reward, done, {\"invalid_action\": action}\n",
    "\n",
    "        old_row, old_col = self.agent_pos\n",
    "        new_row, new_col = old_row, old_col\n",
    "\n",
    "        # Apply movement\n",
    "        if action_enum == Actions.MOVE_UP:\n",
    "            new_row -= 1\n",
    "        elif action_enum == Actions.MOVE_DOWN:\n",
    "            new_row += 1\n",
    "        elif action_enum == Actions.MOVE_LEFT:\n",
    "            new_col -= 1\n",
    "        elif action_enum == Actions.MOVE_RIGHT:\n",
    "            new_col += 1\n",
    "        elif action_enum == Actions.QUIT:\n",
    "            reward = 0.0\n",
    "            done = True\n",
    "            new_state = self.get_local_observation()\n",
    "            return new_state, reward, done, {\"quit\": True}\n",
    "        else:\n",
    "            reward = StepResult.INVALID.value\n",
    "            done = False\n",
    "            new_state = self.get_local_observation()\n",
    "            return new_state, reward, done, {\"unsupported_action\": action_enum.name}\n",
    "\n",
    "        # Increment and check max steps\n",
    "        self.steps_taken += 1\n",
    "        if self.steps_taken >= self.max_steps:\n",
    "            reward = StepResult.INVALID.value\n",
    "            done = True\n",
    "            new_state = self.get_local_observation()\n",
    "            return new_state, reward, done, {\"max_steps_reached\": True}\n",
    "\n",
    "        # Check bounds\n",
    "        if not (0 <= new_row < self.rows and 0 <= new_col < self.cols):\n",
    "            reward = StepResult.HIT_WALL.value\n",
    "            done = False\n",
    "            new_state = self.get_local_observation()\n",
    "            return new_state, reward, done, {\"hit_boundary\": True}\n",
    "\n",
    "        cell_value = self.grid[new_row, new_col]\n",
    "\n",
    "        if cell_value == GridCellValues.WALL:\n",
    "            # Hit a wall, don't move\n",
    "            reward = StepResult.HIT_WALL.value\n",
    "            done = False\n",
    "            new_state = self.get_local_observation()\n",
    "        elif cell_value == GridCellValues.GOAL:\n",
    "            # Found the goal!\n",
    "            self.agent_pos = (new_row, new_col)\n",
    "            self.visited_grid[new_row, new_col] = True\n",
    "            reward = StepResult.GOAL_REACHED.value\n",
    "            done = True\n",
    "            new_state = self.get_local_observation()\n",
    "        else:\n",
    "            # Valid move to empty space\n",
    "            old_distance = abs(old_row - self.goal_pos[0]) + abs(old_col - self.goal_pos[1])\n",
    "            self.agent_pos = (new_row, new_col)\n",
    "            new_distance = abs(new_row - self.goal_pos[0]) + abs(new_col - self.goal_pos[1])\n",
    "            \n",
    "            # Check if this is a new area\n",
    "            is_new_area = not self.visited_grid[new_row, new_col]\n",
    "            self.visited_grid[new_row, new_col] = True\n",
    "            \n",
    "            # Compute reward with distance bonus/penalty\n",
    "            reward = compute_movement_reward(is_new_area, old_distance, new_distance)\n",
    "            done = False\n",
    "            new_state = self.get_local_observation()\n",
    "\n",
    "        return new_state, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e326b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a named tuple to represent a transition in the environment\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1cb9f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(hp: HyperParams, policy_net, target_net, memory, optimizer):\n",
    "    \"\"\"Perform one step of optimization on the policy network.\"\"\"\n",
    "    if len(memory) < hp.batch_size:\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(hp.batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute mask of non-final states\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)), \n",
    "        device=hp.device, \n",
    "        dtype=torch.bool\n",
    "    )\n",
    "    \n",
    "    # Prepare batch tensors first\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    # Handle case where all states are terminal (no non-final states)\n",
    "    non_final_next_states_list = [s for s in batch.next_state if s is not None]\n",
    "    if len(non_final_next_states_list) > 0:\n",
    "        non_final_next_states = torch.cat(non_final_next_states_list)\n",
    "    else:\n",
    "        # Create dummy tensor with correct shape if no non-final states\n",
    "        # Get the observation space size from the first state in the batch\n",
    "        obs_size = state_batch.shape[1]\n",
    "        non_final_next_states = torch.empty((0, obs_size), device=hp.device, dtype=torch.float32)\n",
    "\n",
    "    # Compute Q(s_t, a)\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states\n",
    "    next_state_values = torch.zeros(hp.batch_size, device=hp.device)\n",
    "    if len(non_final_next_states_list) > 0:\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    \n",
    "    # Compute expected Q values\n",
    "    expected_state_action_values = (next_state_values * hp.gamma) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "caa8d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_reward_structure():\n",
    "    \"\"\"Print the complete reward structure for transparency.\"\"\"\n",
    "    print(\"=== Reward Structure ===\")\n",
    "    print(\"Base Action Rewards:\")\n",
    "    for result in StepResult:\n",
    "        print(f\"  {result.name}: {result.value}\")\n",
    "    \n",
    "    print(\"\\nExploration Rewards:\")\n",
    "    print(f\"  NEW_AREA_BONUS: {ExplorationRewards.NEW_AREA_BONUS}\")\n",
    "    print(f\"  REVISIT_PENALTY: {ExplorationRewards.REVISIT_PENALTY}\")\n",
    "    print(f\"  DISTANCE_BONUS: {ExplorationRewards.DISTANCE_BONUS}\")\n",
    "    \n",
    "    print(\"\\nEffective Movement Rewards:\")\n",
    "    print(f\"  Move to new area (closer): {compute_movement_reward(True, 10, 9)}\")\n",
    "    print(f\"  Move to new area (farther): {compute_movement_reward(True, 5, 6)}\")\n",
    "    print(f\"  Move to visited area (closer): {compute_movement_reward(False, 10, 9)}\")\n",
    "    print(f\"  Move to visited area (farther): {compute_movement_reward(False, 5, 6)}\")\n",
    "    print(f\"  Hit wall: {StepResult.HIT_WALL.value}\")\n",
    "    print(f\"  Find goal: {StepResult.GOAL_REACHED.value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b12749c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_environment():\n",
    "    \"\"\"Test the environment in isolation.\"\"\"\n",
    "    print(\"=== Environment Test ===\")\n",
    "    env = DungeonEnv(rows=6, cols=6, wall_prob=0.15, seed=42, device=device)  # Match training env\n",
    "    \n",
    "    print(f\"Environment: {env.rows}x{env.cols} grid, max_steps = {env.max_steps}\")\n",
    "    print(f\"Observation space: {env.observation_space} (local 5x5 view + position)\")\n",
    "    print(f\"Vision range: {env.vision_range}\")\n",
    "    \n",
    "    # Test one episode to show the environment\n",
    "    print(f\"\\n--- Test Episode ---\")\n",
    "    state = env.reset()\n",
    "    print(f\"Start pos: {env.start_pos}, Goal pos: {env.goal_pos}\")\n",
    "    print(f\"Distance to goal: {abs(env.start_pos[0] - env.goal_pos[0]) + abs(env.start_pos[1] - env.goal_pos[1])}\")\n",
    "    print(f\"Initial observation shape: {state.shape}\")\n",
    "    \n",
    "    total_reward = 0\n",
    "    for step in range(10):\n",
    "        # Random exploration\n",
    "        action = env.rng.choice(env.action_space)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        print(f\"  Step {step}: action={action}, reward={reward:.3f}, done={done}\")\n",
    "        print(f\"    agent_pos={env.agent_pos}\")\n",
    "        if info:\n",
    "            print(f\"    info={info}\")\n",
    "        \n",
    "        if done:\n",
    "            print(f\"    Episode ended: {info}\")\n",
    "            break\n",
    "        elif step == 9:\n",
    "            print(f\"    Episode continued...\")\n",
    "    \n",
    "    # Show final grid\n",
    "    print(f\"  Grid visualization:\")\n",
    "    for i in range(env.rows):\n",
    "        row_str = \"    \"\n",
    "        for j in range(env.cols):\n",
    "            if (i, j) == env.agent_pos:\n",
    "                row_str += \"A \"\n",
    "            elif (i, j) == env.goal_pos:\n",
    "                row_str += \"G \"\n",
    "            elif env.grid[i, j] == GridCellValues.WALL:\n",
    "                row_str += \"# \"\n",
    "            elif env.visited_grid[i, j]:\n",
    "                row_str += \". \"\n",
    "            else:\n",
    "                row_str += \"  \"\n",
    "        print(row_str)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d77828b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn():\n",
    "    \"\"\"Main training function.\"\"\"\n",
    "    # Print reward structure for clarity\n",
    "    print_reward_structure()\n",
    "    \n",
    "    # Run environment test first\n",
    "    test_environment()\n",
    "    \n",
    "    # Set seeds for reproducibility\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    # Initialize environment and hyperparameters\n",
    "    env = DungeonEnv(rows=6, cols=6, wall_prob=0.15, device=device)  # Start smaller and easier\n",
    "    hp = HyperParams()\n",
    "    \n",
    "    n_actions = len(env.action_space)\n",
    "    n_observations = env.observation_space\n",
    "\n",
    "    print(f\"=== Training Setup ===\")\n",
    "    print(f\"Environment: {env.rows}x{env.cols} grid, max_steps = {env.max_steps}\")\n",
    "    print(f\"Action space: {env.action_space}\")\n",
    "    print(f\"Observation space: {n_observations}\")\n",
    "    print(f\"Learning rate: {hp.lr}, Epsilon decay: {hp.eps_decay}\")\n",
    "\n",
    "    # Initialize networks with improved architecture\n",
    "    policy_net = DQN(n_observations, n_actions).to(hp.device)\n",
    "    target_net = DQN(n_observations, n_actions).to(hp.device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    # Set networks to appropriate modes\n",
    "    policy_net.train()  # Enable training mode for batch norm and dropout\n",
    "    target_net.eval()   # Keep target network in eval mode\n",
    "\n",
    "    # Initialize agent, optimizer, and memory\n",
    "    dqn_agent = DQNAgent(policy_net=policy_net, hp=hp)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=hp.lr)  # Adam often works better than AdamW\n",
    "    memory = ReplayMemory(50000)  # Much larger memory\n",
    "\n",
    "    # Training loop\n",
    "    num_episodes = 2000  # More episodes for the improved setup\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    goals_reached = 0\n",
    "    exploration_stats = []\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        # Initialize environment\n",
    "        state = env.reset()\n",
    "        state = state.unsqueeze(0)  # Add batch dimension\n",
    "        total_reward = 0\n",
    "        episode_length = 0\n",
    "        goal_reached_this_episode = False\n",
    "        cells_explored = 0\n",
    "\n",
    "        for t in count():\n",
    "            # Select action\n",
    "            action = dqn_agent.select_action(state, env.action_space)\n",
    "            \n",
    "            # Take step\n",
    "            next_state, reward, done, info = env.step(action.item())\n",
    "            total_reward += reward\n",
    "            episode_length += 1\n",
    "            \n",
    "            # Count exploration\n",
    "            cells_explored = np.sum(env.visited_grid)\n",
    "            \n",
    "            # Check if goal was reached\n",
    "            if reward >= 10.0:\n",
    "                goal_reached_this_episode = True\n",
    "            \n",
    "            # Prepare tensors\n",
    "            next_state = next_state.unsqueeze(0) if not done else None\n",
    "            reward_tensor = torch.tensor([reward], device=hp.device, dtype=torch.float32)\n",
    "\n",
    "            # Store transition\n",
    "            memory.push(state, action, next_state, reward_tensor)\n",
    "\n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "\n",
    "            # Optimize model (start learning earlier with better architecture)\n",
    "            if len(memory) > hp.batch_size * 4:  # Start after 4 batches worth of experience\n",
    "                optimize_model(hp, policy_net, target_net, memory, optimizer)\n",
    "\n",
    "            # Soft update target network\n",
    "            target_net_state_dict = target_net.state_dict()\n",
    "            policy_net_state_dict = policy_net.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = (\n",
    "                    policy_net_state_dict[key] * hp.tau + \n",
    "                    target_net_state_dict[key] * (1 - hp.tau)\n",
    "                )\n",
    "            target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "            if done:\n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_lengths.append(episode_length)\n",
    "                exploration_stats.append(cells_explored)\n",
    "                if goal_reached_this_episode:\n",
    "                    goals_reached += 1\n",
    "                \n",
    "                # Enhanced logging every 50 episodes for faster feedback\n",
    "                if i_episode % 50 == 0:\n",
    "                    avg_reward = np.mean(episode_rewards[-100:])\n",
    "                    avg_length = np.mean(episode_lengths[-100:])\n",
    "                    avg_exploration = np.mean(exploration_stats[-100:])\n",
    "                    goal_rate = goals_reached / (i_episode + 1) * 100\n",
    "                    current_eps = hp.eps_end + (hp.eps_start - hp.eps_end) * math.exp(-1. * dqn_agent.steps_done / hp.eps_decay)\n",
    "                    \n",
    "                    print(f\"Episode {i_episode}\")\n",
    "                    print(f\"  Average Reward: {avg_reward:.2f}\")\n",
    "                    print(f\"  Average Length: {avg_length:.1f}\")\n",
    "                    print(f\"  Average Exploration: {avg_exploration:.1f}/{env.rows * env.cols} cells\")\n",
    "                    print(f\"  Goals Reached: {goals_reached}/{i_episode + 1} ({goal_rate:.1f}%)\")\n",
    "                    print(f\"  Epsilon: {current_eps:.3f}\")\n",
    "                    if i_episode > 0:\n",
    "                        print(f\"  Last Episode Info: {info}\")\n",
    "                    print()\n",
    "                break\n",
    "\n",
    "    return policy_net, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c0212b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Reward Structure ===\n",
      "Base Action Rewards:\n",
      "  MOVED: 0.1\n",
      "  HIT_WALL: -0.2\n",
      "  GOAL_REACHED: 50.0\n",
      "  INVALID: -0.1\n",
      "\n",
      "Exploration Rewards:\n",
      "  NEW_AREA_BONUS: 1.0\n",
      "  REVISIT_PENALTY: -0.05\n",
      "  DISTANCE_BONUS: 0.1\n",
      "\n",
      "Effective Movement Rewards:\n",
      "  Move to new area (closer): 1.2000000000000002\n",
      "  Move to new area (farther): 1.0\n",
      "  Move to visited area (closer): 0.15000000000000002\n",
      "  Move to visited area (farther): -0.05\n",
      "  Hit wall: -0.2\n",
      "  Find goal: 50.0\n",
      "\n",
      "=== Environment Test ===\n",
      "Environment: 6x6 grid, max_steps = 108\n",
      "Observation space: 27 (local 5x5 view + position)\n",
      "Vision range: 2\n",
      "\n",
      "--- Test Episode ---\n",
      "Start pos: (np.int64(1), np.int64(1)), Goal pos: (np.int64(4), np.int64(4))\n",
      "Distance to goal: 6\n",
      "Initial observation shape: torch.Size([27])\n",
      "  Step 0: action=1, reward=-0.200, done=False\n",
      "    agent_pos=(np.int64(1), np.int64(1))\n",
      "  Step 1: action=1, reward=-0.200, done=False\n",
      "    agent_pos=(np.int64(1), np.int64(1))\n",
      "  Step 2: action=0, reward=-0.200, done=False\n",
      "    agent_pos=(np.int64(1), np.int64(1))\n",
      "  Step 3: action=1, reward=-0.200, done=False\n",
      "    agent_pos=(np.int64(1), np.int64(1))\n",
      "  Step 4: action=0, reward=-0.200, done=False\n",
      "    agent_pos=(np.int64(1), np.int64(1))\n",
      "  Step 5: action=0, reward=-0.200, done=False\n",
      "    agent_pos=(np.int64(1), np.int64(1))\n",
      "  Step 6: action=1, reward=-0.200, done=False\n",
      "    agent_pos=(np.int64(1), np.int64(1))\n",
      "  Step 7: action=3, reward=1.200, done=False\n",
      "    agent_pos=(np.int64(1), np.int64(2))\n",
      "  Step 8: action=3, reward=1.200, done=False\n",
      "    agent_pos=(np.int64(1), np.int64(3))\n",
      "  Step 9: action=3, reward=1.200, done=False\n",
      "    agent_pos=(np.int64(1), np.int64(4))\n",
      "    Episode continued...\n",
      "  Grid visualization:\n",
      "    # # # # # # \n",
      "    # . . . A # \n",
      "    # #       # \n",
      "    #         # \n",
      "    #       G # \n",
      "    # # # # # # \n",
      "\n",
      "=== Training Setup ===\n",
      "Environment: 6x6 grid, max_steps = 108\n",
      "Action space: [0, 1, 2, 3]\n",
      "Observation space: 27\n",
      "Learning rate: 0.0005, Epsilon decay: 15000\n",
      "Episode 0\n",
      "  Average Reward: 54.20\n",
      "  Average Length: 32.0\n",
      "  Average Exploration: 8.0/36 cells\n",
      "  Goals Reached: 1/1 (100.0%)\n",
      "  Epsilon: 0.998\n",
      "\n",
      "Episode 50\n",
      "  Average Reward: 14.21\n",
      "  Average Length: 90.5\n",
      "  Average Exploration: 9.5/36 cells\n",
      "  Goals Reached: 15/51 (29.4%)\n",
      "  Epsilon: 0.740\n",
      "  Last Episode Info: {'max_steps_reached': True}\n",
      "\n",
      "Episode 100\n",
      "  Average Reward: 19.52\n",
      "  Average Length: 86.0\n",
      "  Average Exploration: 10.0/36 cells\n",
      "  Goals Reached: 36/101 (35.6%)\n",
      "  Epsilon: 0.571\n",
      "  Last Episode Info: {}\n",
      "\n",
      "Episode 150\n",
      "  Average Reward: 28.16\n",
      "  Average Length: 78.3\n",
      "  Average Exploration: 11.3/36 cells\n",
      "  Goals Reached: 59/151 (39.1%)\n",
      "  Epsilon: 0.447\n",
      "  Last Episode Info: {'max_steps_reached': True}\n",
      "\n",
      "Episode 200\n",
      "  Average Reward: 22.61\n",
      "  Average Length: 82.2\n",
      "  Average Exploration: 10.5/36 cells\n",
      "  Goals Reached: 74/201 (36.8%)\n",
      "  Epsilon: 0.338\n",
      "  Last Episode Info: {'max_steps_reached': True}\n",
      "\n",
      "Episode 250\n",
      "  Average Reward: 18.15\n",
      "  Average Length: 86.6\n",
      "  Average Exploration: 9.7/36 cells\n",
      "  Goals Reached: 91/251 (36.3%)\n",
      "  Epsilon: 0.260\n",
      "  Last Episode Info: {'max_steps_reached': True}\n",
      "\n",
      "Episode 300\n",
      "  Average Reward: 17.96\n",
      "  Average Length: 85.4\n",
      "  Average Exploration: 9.6/36 cells\n",
      "  Goals Reached: 106/301 (35.2%)\n",
      "  Epsilon: 0.200\n",
      "  Last Episode Info: {'max_steps_reached': True}\n",
      "\n",
      "Episode 350\n",
      "  Average Reward: 11.52\n",
      "  Average Length: 91.0\n",
      "  Average Exploration: 9.0/36 cells\n",
      "  Goals Reached: 113/351 (32.2%)\n",
      "  Epsilon: 0.151\n",
      "  Last Episode Info: {'max_steps_reached': True}\n",
      "\n",
      "Episode 400\n",
      "  Average Reward: 10.21\n",
      "  Average Length: 94.5\n",
      "  Average Exploration: 9.1/36 cells\n",
      "  Goals Reached: 124/401 (30.9%)\n",
      "  Epsilon: 0.116\n",
      "  Last Episode Info: {'max_steps_reached': True}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trained_net, rewards = \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Plot training progress with moving average\u001b[39;00m\n\u001b[32m      4\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m8\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mtrain_dqn\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Optimize model (start learning earlier with better architecture)\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(memory) > hp.batch_size * \u001b[32m4\u001b[39m:  \u001b[38;5;66;03m# Start after 4 batches worth of experience\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# Soft update target network\u001b[39;00m\n\u001b[32m     90\u001b[39m target_net_state_dict = target_net.state_dict()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36moptimize_model\u001b[39m\u001b[34m(hp, policy_net, target_net, memory, optimizer)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(non_final_next_states_list) > \u001b[32m0\u001b[39m:\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         next_state_values[non_final_mask] = \u001b[43mtarget_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnon_final_next_states\u001b[49m\u001b[43m)\u001b[49m.max(\u001b[32m1\u001b[39m).values\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Compute expected Q values\u001b[39;00m\n\u001b[32m     41\u001b[39m expected_state_action_values = (next_state_values * hp.gamma) + reward_batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dev\\Dropbox\\GitHub\\ai-dungeon-crawl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dev\\Dropbox\\GitHub\\ai-dungeon-crawl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mDQN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     51\u001b[39m x = F.relu(\u001b[38;5;28mself\u001b[39m.fc2(x))\n\u001b[32m     52\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dropout(x)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m x = F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     54\u001b[39m x = \u001b[38;5;28mself\u001b[39m.fc4(x)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dev\\Dropbox\\GitHub\\ai-dungeon-crawl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dev\\Dropbox\\GitHub\\ai-dungeon-crawl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dev\\Dropbox\\GitHub\\ai-dungeon-crawl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trained_net, rewards = train_dqn()\n",
    "\n",
    "# Plot training progress with moving average\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(rewards, alpha=0.3, label='Episode Reward')\n",
    "\n",
    "# Calculate moving average\n",
    "window_size = min(50, len(rewards) // 4)  # Adaptive window size\n",
    "if len(rewards) >= window_size and window_size > 1:\n",
    "    moving_avg = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.plot(range(window_size-1, len(rewards)), moving_avg, label=f'Moving Average ({window_size} episodes)')\n",
    "\n",
    "plt.title('Training Progress - Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "# Plot running success rate (rough estimate)\n",
    "success_threshold = 30  # Assume success if reward > 30 (close to goal reward)\n",
    "successes = [1 if r > success_threshold else 0 for r in rewards]\n",
    "if len(successes) >= window_size and window_size > 1:\n",
    "    success_rate = np.convolve(successes, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.plot(range(window_size-1, len(rewards)), success_rate * 100)\n",
    "    plt.title('Estimated Success Rate')\n",
    "    plt.ylabel('Success Rate (%)')\n",
    "else:\n",
    "    plt.plot(successes)\n",
    "    plt.title('Success/Failure per Episode')\n",
    "    plt.ylabel('Success (1) / Failure (0)')\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Final average reward (last 100 episodes): {np.mean(rewards[-100:]):.2f}\")\n",
    "print(f\"Best episode reward: {max(rewards):.2f}\")\n",
    "print(f\"Episodes with reward > {success_threshold}: {sum(successes)}/{len(successes)} ({sum(successes)/len(successes)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6057572f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-dungeon-crawl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
