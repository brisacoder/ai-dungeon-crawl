{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a48a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import math\n",
    "import random\n",
    "from enum import Enum, auto\n",
    "from dataclasses import dataclass, field\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aba9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class HyperParams:\n",
    "    batch_size: int = 32\n",
    "    gamma: float = 0.99\n",
    "    eps_start: float = 0.9\n",
    "    eps_end: float = 0.01\n",
    "    eps_decay: int = 2500\n",
    "    tau: float = 0.005\n",
    "    lr: float = 3e-4\n",
    "    device: torch.device = field(default_factory=lambda: device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d868bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ac2a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepResult(Enum):\n",
    "    MOVED = -0.1\n",
    "    HIT_WALL = -1.0\n",
    "    KEY_PICKED = 2.0\n",
    "    DOOR_OPENED = 1.0\n",
    "    DOOR_BLOCKED = -1.0\n",
    "    GOAL_REACHED = 10.0\n",
    "    INVALID = 0.0  # fallback or unused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7ab166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(result: StepResult) -> float:\n",
    "    return result.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de5c1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actions(Enum):\n",
    "    \"\"\"\n",
    "    Enum for actions in the dungeon environment.\n",
    "    \"\"\"\n",
    "    MOVE_UP = 0\n",
    "    MOVE_DOWN = 1\n",
    "    MOVE_LEFT = 2\n",
    "    MOVE_RIGHT = 3\n",
    "    ATTACK = 4\n",
    "    PICKUP_ITEM = 5\n",
    "    USE_ITEM = 6\n",
    "    OPEN_INVENTORY = 7\n",
    "    QUIT = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0800430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, policy_net, hp):\n",
    "        self.policy_net = policy_net\n",
    "        self.hp = hp\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def select_action(self, state, action_space):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.hp.EPS_END + (self.hp.EPS_START - self.hp.EPS_END) * \\\n",
    "            math.exp(-1. * self.steps_done / self.hp.EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                # t.max(1) will return the largest column value of each row.\n",
    "                # second column on max result is index of where max element was\n",
    "                # found, so we pick action with the larger expected reward.\n",
    "                return self.policy_net(state).max(1).indices.view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.choice(action_space)]], device=self.hp.device, dtype=torch.long)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1d8aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridCellValues(Enum):\n",
    "    EMPTY=0\n",
    "    WALL=auto()\n",
    "    VISITED=auto()\n",
    "    GOAL=auto()\n",
    "    PATH=auto()   \n",
    "    START=auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10552dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DungeonEnv:\n",
    "    def __init__(self, rows=5, cols=5, wall_prob=0.3, seed=None, device=torch.device(\"cpu\")):\n",
    "        self.action_space = [a.value for a in Actions if a.value < Actions.ATTACK.value]\n",
    "        self.observation_space = 2\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.wall_prob = wall_prob\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.grid = None\n",
    "        self.start_pos = None\n",
    "        self.goal_pos = None\n",
    "        self.agent_pos = None\n",
    "        self.device = device\n",
    "        self.reset()\n",
    "     \n",
    "    def reset(self):\n",
    "        self.grid = self.rng.choice([GridCellValues.EMPTY, GridCellValues.WALL],\n",
    "                                     (self.rows, self.cols), \n",
    "                                     p=[1 - self.wall_prob, self.wall_prob]) \n",
    "        # Find all empty cells *before* placing start/goal\n",
    "        empty_cells = list(zip(*np.where(self.grid == GridCellValues.EMPTY)))\n",
    "        # Pick goal position\n",
    "        self.goal_pos = self.rng.choice(len(empty_cells))\n",
    "        goal_cell = empty_cells[self.goal_pos]\n",
    "\n",
    "        # Remove goal cell from list before picking start\n",
    "        empty_cells.pop(self.goal_pos)\n",
    "        start_cell = empty_cells[self.rng.choice(len(empty_cells))]\n",
    "\n",
    "        # Set in grid\n",
    "        self.grid[goal_cell] = GridCellValues.GOAL\n",
    "        self.grid[start_cell] = GridCellValues.START\n",
    "        self.start_pos = start_cell\n",
    "        self.agent_pos = start_cell \n",
    "        # Return state as [1, 2] tensor\n",
    "        row, col = self.agent_pos\n",
    "        state = torch.tensor([[row, col]], device=self.device, dtype=torch.float32)  # shape: [1, 2]\n",
    "        return state\n",
    "\n",
    "\n",
    "    def step(self, action: int):\n",
    "        \"\"\"\n",
    "        Apply an action in the grid and return the next state, reward, and done flag.\n",
    "\n",
    "        Args:\n",
    "            action (int): Integer corresponding to an Actions enum value.\n",
    "\n",
    "        Returns:\n",
    "            new_state (torch.Tensor): The new position as a 2D int tensor [row, col]\n",
    "            reward (float): The reward for this action\n",
    "            done (bool): True if the episode has ended\n",
    "            info (dict): Optional debug information\n",
    "        \"\"\"\n",
    "        try:\n",
    "            action_enum = Actions(action)\n",
    "        except ValueError:\n",
    "            # Invalid enum value passed in\n",
    "            reward = StepResult.INVALID.value\n",
    "            done = False\n",
    "            new_state = torch.tensor(self.agent_pos, device=self.device, dtype=torch.int32)\n",
    "            return new_state, reward, done, {\"invalid_action\": action}\n",
    "\n",
    "        row, col = self.agent_pos\n",
    "        new_row, new_col = row, col\n",
    "\n",
    "        # Apply movement\n",
    "        match action_enum:\n",
    "            case Actions.MOVE_UP:\n",
    "                new_row -= 1\n",
    "            case Actions.MOVE_DOWN:\n",
    "                new_row += 1\n",
    "            case Actions.MOVE_LEFT:\n",
    "                new_col -= 1\n",
    "            case Actions.MOVE_RIGHT:\n",
    "                new_col += 1\n",
    "            case Actions.QUIT:\n",
    "                reward = 0.0\n",
    "                done = True\n",
    "                new_state = torch.tensor([row, col], device=self.device, dtype=torch.int32)\n",
    "                return new_state, reward, done, {\"quit\": True}\n",
    "            case _:\n",
    "                reward = StepResult.INVALID.value\n",
    "                done = False\n",
    "                new_state = torch.tensor([row, col], device=self.device, dtype=torch.int32)\n",
    "                return new_state, reward, done, {\"unsupported_action\": action_enum.name}\n",
    "\n",
    "        # Check bounds\n",
    "        if not (0 <= new_row < self.rows and 0 <= new_col < self.cols):\n",
    "            reward = StepResult.HIT_WALL.value\n",
    "            done = False\n",
    "            new_state = torch.tensor([row, col], device=self.device, dtype=torch.int32)\n",
    "            return new_state, reward, done, {}\n",
    "\n",
    "        cell_value = self.grid[new_row, new_col]\n",
    "\n",
    "        match cell_value:\n",
    "            case GridCellValues.WALL:\n",
    "                reward = StepResult.HIT_WALL.value\n",
    "                done = False\n",
    "                new_state = torch.tensor([row, col], device=self.device, dtype=torch.int32)\n",
    "\n",
    "            case GridCellValues.GOAL:\n",
    "                self.agent_pos = (new_row, new_col)\n",
    "                reward = StepResult.GOAL_REACHED.value\n",
    "                done = True\n",
    "                new_state = torch.tensor([new_row, new_col], device=self.device, dtype=torch.int32)\n",
    "\n",
    "            case _:\n",
    "                self.agent_pos = (new_row, new_col)\n",
    "                reward = StepResult.MOVED.value\n",
    "                done = False\n",
    "                new_state = torch.tensor([new_row, new_col], device=self.device, dtype=torch.int32)\n",
    "\n",
    "        return new_state, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e326b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a named tuple to represent a transition in the environment\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cb9f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(hp: HyperParams, policy_net, target_net, memory, optimizer):\n",
    "    if len(memory) < hp.batch_size:\n",
    "        return\n",
    "    transitions = memory.sample(hp.batch_size)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(hp.batch_size, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * hp.gamma) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d77828b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x1e7dd72a000>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# To ensure reproducibility during training, you can fix the random seeds\n",
    "# by uncommenting the lines below. This makes the results consistent across\n",
    "# runs, which is helpful for debugging or comparing different approaches.\n",
    "#\n",
    "# That said, allowing randomness can be beneficial in practice, as it lets\n",
    "# the model explore different training trajectories.\n",
    "\n",
    "\n",
    "# seed = 42\n",
    "# random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "# env.reset(seed=seed)\n",
    "# env.action_space.seed(seed)\n",
    "# env.observation_space.seed(seed)\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed(seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8087c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DungeonEnv()\n",
    "hp = HyperParams()\n",
    "n_actions = len(env.action_space)\n",
    "# Get the number of state observations\n",
    "state = env.reset()\n",
    "n_observations = state.size\n",
    "policy_net = DQN(n_observations, n_actions).to(hp.device)\n",
    "target_net = DQN(n_observations, n_actions).to(hp.device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "dqn_agent = DQNAgent(policy_net=policy_net, hp=hp)\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=hp.lr, amsgrad=True)\n",
    "memory = ReplayMemory(1000)\n",
    "\n",
    "action = dqn_agent.select_action(state, env.action_space)\n",
    "next_state, reward, done, info = env.step(action)\n",
    "# Convert next_state to a batch of size 1\n",
    "next_state = next_state.unsqueeze(0)\n",
    "reward = torch.tensor([[reward]], device=hp.device, dtype=torch.float32)\n",
    "action = torch.tensor([[action]], device=hp.device, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4e79d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-dungeon-crawl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
